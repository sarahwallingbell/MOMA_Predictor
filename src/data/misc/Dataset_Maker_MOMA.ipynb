{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files\n",
    "- All files have already been created. You can choose to not run this notebook. If you do run it, the files will just be overwritten. Nothing will change, so go for it!\n",
    "- I manually edited the largest data file due to some weird comma-parsing errors.\n",
    "\n",
    "# Final Dataset Format\n",
    "### The dataset file with all information is named \"dataset.csv\".\n",
    "\n",
    "The attributes in the file are comma separated.\n",
    "\n",
    "The order of the attributes is:\n",
    "1. Number of nouns\n",
    "2. Number of foreign words\n",
    "3. Number of prepositions\n",
    "4. Number of determiners\n",
    "5. Number of adjectives\n",
    "6. Nationality of artist\n",
    "7. Gender of artist\n",
    "\n",
    "### The dataset file with gender only is named \"gender_dataset.csv\".\n",
    "\n",
    "The attributes in the file are comma separated.\n",
    "\n",
    "The order of the attributes is:\n",
    "\n",
    "1. Number of nouns\n",
    "2. Number of foreign words\n",
    "3. Number of prepositions\n",
    "4. Number of determiners\n",
    "5. Number of adjectives\n",
    "6. Gender of artist\n",
    "\n",
    "### The dataset file with nationality only is named \"nationality_dataset.csv\".\n",
    "\n",
    "The attributes in the file are comma separated.\n",
    "\n",
    "The order of the attributes is:\n",
    "\n",
    "1. Number of nouns\n",
    "2. Number of foreign words\n",
    "3. Number of prepositions\n",
    "4. Number of determiners\n",
    "5. Number of adjectives\n",
    "6. Nationality of artist\n",
    "\n",
    "Note: we only use gender_dataset.csv, as our goal is to predict artist gender from titles, however we create a dataset for predicting artist nationality in the future. \n",
    "\n",
    "# Downloading the dataset\n",
    "You don't need to do this step!\n",
    "First, I created a directory called \"dataset.\"\n",
    "\n",
    "From inside the dataset directory, I downloaded the MOMA dataset.\n",
    "\n",
    "These files are too large to be pushed to Github. Here are the commands I ran:\n",
    "\n",
    "mkdir dataset\n",
    "\n",
    "git clone https://github.com/MuseumofModernArt/collection\n",
    "\n",
    "# Download NLTK\n",
    "- Download Python 3.7 (https://www.python.org/downloads/)\n",
    "- Install numpy by running this command: pip install numpy\n",
    "- Install NLTK by running this command: pip install nltk\n",
    "\n",
    "# Parse Dataset\n",
    "We only want to keep three things from our dataset: the title, the gender, and the nationality. We are using the data.csv file.\n",
    "\n",
    "The data.csv file is a file I made some manual edits to in Excel for ease- I deleted some columns and punctuation.\n",
    "\n",
    "Notice the title is the first element in each line. The nationality is the fifth element in every line. The gender is the eighth element in every line.\n",
    "\n",
    "The script below opens the dataset and pulls just the data we want from every line. Then it writes it a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "#Opening and Making Files\n",
    "##########################\n",
    "\n",
    "# Open the file, which we will name 'file'\n",
    "# If you run this yourself, make sure Artworks.csv is in the same directory or change the file path\n",
    "# The 'r' parameter is saying that we only need to read this file, not write to it\n",
    "file = open(\"data.csv\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# Lines is a Python list (like an array) of the lines in the file\n",
    "lines = file.readlines()\n",
    "\n",
    "# We also need to make a new file to write our data to.\n",
    "# \"w+\" is how we tell Python that we are writing to this file (the '+' means create it if it doesn't already exist)\n",
    "new_file = open(\"temp_dataset.txt\", \"w+\", encoding=\"utf-8\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Title,Nationality,Gender\n",
      "\n",
      "Ferdinandsbrücke Project Vienna Austria Elevation preliminary version,Austrian,Male\n",
      "\n",
      "The Manhattan Transcripts Project New York New York Episode 1: The Park,,Male\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Practice with Lines\n",
    "##########################\n",
    "\n",
    "# We can access the elements in the list of lines.\n",
    "# For example, this code will print out the first line:\n",
    "print (lines[0])\n",
    "# The first line tells us what the columns mean! \n",
    "# That's convenient- looks like we need column 0, 4, and 7.\n",
    "# Here is the first line of actual data:\n",
    "print (lines[1])\n",
    "# Now look at line 8:\n",
    "print (lines[8])\n",
    "# Some of our data is missing Nationalities or genders!\n",
    "# We need to deal with this. \n",
    "# Since we have a lot of data for a decision tree, I just removed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Writing our data to a new file\n",
    "################################\n",
    "\n",
    "# This is a for each loop that goes over each line.\n",
    "for line in lines:\n",
    "    #split each line on commas\n",
    "    elements = line.split(',')\n",
    "    # Was all the information there?\n",
    "    if (len(elements) == 3):\n",
    "        # Check one more time because Python reasons\n",
    "        if (elements[0] != \"\" and elements[1] != \"\" and elements[2] != \"\"):\n",
    "            # Sometimes more than one nationality was listed. Let's take the first one.\n",
    "            nationality = elements[1].split(\" \")[0]\n",
    "            # Sometimes more than one gender was listed. Let's take the first one.\n",
    "            gender = elements[2].split(\" \")[0]\n",
    "            # Sometimes random nonsense got into our data. Let's make sure the gender is 'valid' (i.e. in this dataset)\n",
    "            if (gender == \"Male\" or gender ==\"Female\"):\n",
    "                new_line = elements[0] + \",\" + nationality + \",\" + gender + \"\\n\"\n",
    "                new_file.write(new_line)\n",
    "# Close our files!\n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NLTK\n",
    "Here is the outline of what we will do now:\n",
    "\n",
    "1. import the Natural Language ToolKit.\n",
    "2. Go through each line of new file.\n",
    "3. Tokenize and Tag the title on each line.\n",
    "4. Use the tagging to count the FOREIGN WORDS (FW), NOUNS (NN or NNS or NNP or NNPS), PREPOSITIONS (IN), ADJECTIVES (JJ, JJR, JJS), and DETERMINERS (DT).\n",
    "5. Write this information to a new csv file.\n",
    "\n",
    "### Explanation of Tags:\n",
    "- FW is any foreign word (this may impact our results! We are using Anglocentric software...Perhaps our model will be excellent at predicting American/British nationalities)\n",
    "- NN is a singular noun. NNS is a plural noun. NNP is a singular proper noun. NNPS is a plural proper noun.\n",
    "- IN is a preposition.\n",
    "- JJ is a regular adjective (big). JJR is a comparative adjective (bigger). JJS is a superlative adjective (biggest).\n",
    "- DT is a determiner (e.g. \"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sarah/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/Users/Sarah/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/Users/Sarah/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/Users/Sarah/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/Users/Sarah/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/Sarah/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/Sarah/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/Users/Sarah/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/Sarah/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/Sarah/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "[nltk_data] Downloading package punkt to /Users/Sarah/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Sarah/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NLTK\n",
    "import nltk\n",
    "# Download some extra stuff\n",
    "# I promise I'm not actually punking you\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open up our temp data file\n",
    "old_data = open(\"temp_dataset.txt\", \"r\", encoding=\"utf-8\")\n",
    "old_lines = old_data.readlines()\n",
    "old_data.close()\n",
    "# Make our final dataset file\n",
    "dataset = open(\"dataset.csv\", \"w+\", encoding=\"utf-8\")\n",
    "gender_dataset = open(\"gender_dataset.csv\", \"w+\", encoding=\"utf-8\")\n",
    "nat_dataset = open(\"nationality_dataset.csv\", \"w+\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seagull - Bikini of God,American,Male\n",
      "\n",
      "Seagull - Bikini of God\n",
      "[('Seagull', 'NNP'), ('-', ':'), ('Bikini', 'NNP'), ('of', 'IN'), ('God', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Practice with NLTK\n",
    "####################\n",
    "\n",
    "#Grab a random line from our dataset to play with\n",
    "temp = old_lines[1329]\n",
    "print(old_lines[1329])\n",
    "#Split the line on commas\n",
    "temp_elements = temp.split(\",\")\n",
    "#Print the title we are playing with\n",
    "print(temp_elements[0])\n",
    "#Tokenize the title\n",
    "text = nltk.word_tokenize(temp_elements[0])\n",
    "#Tag the title\n",
    "tags = nltk.pos_tag(text)\n",
    "\n",
    "# tags is a tuple (basically a 2D array)\n",
    "# tags[X][0] is the word we tagged, tags[X][1] is the POS tag\n",
    "# for example, tags[0][1] is the tag for the first word in the title\n",
    "\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some notes on NLTK tagging\n",
    "Wow, sometimes the sentence tagging is AWFUL\n",
    "Options:\n",
    "1. Tag everything ourselves\n",
    "2. Ignore the problem\n",
    "3. Justify the problem\n",
    "\n",
    "Option 3: Well, it's bad, but... our research doesn't necessarily rely on the tagging being accurate, just consistent. Ultimately, it doesn't matter if NLTK things something is a noun and it isn't as long as it's consistent. We are looking for patterns. We should acknoledge that this means we can't make claims about noun usage in women's art titles in general, only through the lens of NLTK. Part of our presentation could be about limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each line\n",
    "for line in old_lines:\n",
    "    \n",
    "    NN = 0 # number of nouns\n",
    "    FW = 0 # number of foreign words\n",
    "    IN = 0 # number of prepositions\n",
    "    DET = 0 # number of determiners\n",
    "    ADJ = 0 # number of adjectives\n",
    "    \n",
    "    # get the elements from each line in our temp_data file\n",
    "    elements = line.split(\",\")\n",
    "    \n",
    "    # tokenize and tag the title\n",
    "    text = nltk.word_tokenize(elements[0])\n",
    "    tags = nltk.pos_tag(text)\n",
    "    \n",
    "    # go through each POS tag and count tags\n",
    "    for tag in tags:\n",
    "        if (tag[1] == \"NN\" or tag[1] == \"NNS\" or tag[1] == \"NNP\" or tag[1] == \"NNPS\"):\n",
    "            if(NN < 20):\n",
    "                NN = NN + 1\n",
    "        if (tag[1] == \"FW\"):\n",
    "            if(FW < 1):\n",
    "                FW = FW + 1\n",
    "        if (tag[1] == \"IN\"):\n",
    "            if(IN < 1):\n",
    "                IN = IN + 1\n",
    "        if (tag[1] == \"DET\"):\n",
    "            if(DET < 1):\n",
    "                DET = DET + 1\n",
    "        if (tag[1] == \"JJ\" or tag[1] == \"JJR\" or tag[1] == \"JJS\"):\n",
    "            if(ADJ < 1):\n",
    "                ADJ = ADJ + 1\n",
    "    write_line = str(NN) + \",\" + str(FW) + \",\" + str(IN) + \",\" + str(DET) + \",\" + str(ADJ) + elements[1] + elements[2]\n",
    "    dataset.write(write_line)\n",
    "    write_line = str(NN) + \",\" + str(FW) + \",\" + str(IN) + \",\" + str(DET) + \",\" + str(ADJ) + \",\" + elements[2]\n",
    "    gender_dataset.write(write_line)\n",
    "    write_line = str(NN) + \",\" + str(FW) + \",\" + str(IN) + \",\" + str(DET) + \",\" + str(ADJ) + \",\" + elements[1] + \"\\n\"\n",
    "    nat_dataset.write(write_line)\n",
    "\n",
    "#close data file\n",
    "dataset.close()\n",
    "gender_dataset.close()\n",
    "nat_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
